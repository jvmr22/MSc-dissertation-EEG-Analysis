{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import DGCNN\n",
    "from sklearn import preprocessing\n",
    "from utils import generating_data\n",
    "from glob import glob\n",
    "\n",
    "lr_dgcnn = 0.01\n",
    "weight_decay_dgcnn = 5e-4 #tem no codigo do GMSS tmbm\n",
    "K=2\n",
    "epochs=20 # DEFFERARD TMBM USA 20\n",
    "device = 'cpu'\n",
    "\n",
    "hiperparam_run = {\n",
    "    'lr_dgcnn ':lr_dgcnn,\n",
    "    'weight_decay_dgcnn':weight_decay_dgcnn,\n",
    "    'K':K,\n",
    "    'epochs':epochs\n",
    "}\n",
    "\n",
    "label_list = np.array([1,0,-1,-1,0,1,-1,0,1,1,0,-1,0,1,-1]) + 1 # -1 for \n",
    "#fn = 'de_LDS'\n",
    "feature_list = ['de_LDS','psd_LDS', 'rasm_LDS', 'dasm_LDS', 'asm_LDS','dcau_LDS']\n",
    "\n",
    "full_path = \"/home/joaovmrf/Documents/mestrado/SEED/SEED_EEG/SEED_EEG/ExtractedFeatures\"\n",
    "all_files = [each_file for each_file in glob(full_path+'/*.mat') if 'label.mat' not in each_file]\n",
    "all_files.sort()\n",
    "# all_files = glob.glob(full_path+\"*\")\n",
    "\n",
    "all_acc_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cur_file in all_files:\n",
    "cur_file = all_files[0]\n",
    "# for fn in feature_list:\n",
    "fn = 'dasm_LDS'\n",
    "#item = \"15_20130709.mat\"\n",
    "#item = \"13_20140527.mat\"\n",
    "item = os.path.basename(cur_file)\n",
    "cur_user = item.split(\"_\")[0]\n",
    "user_session = datetime.strptime(item.split(\"_\")[1][:-4],\"%Y%m%d\")\n",
    "print(f\"- reading file: {item}\\n--user:{cur_user}, sessions: {user_session}\")\n",
    "\n",
    "# cur_file = os.path.join(full_path, item)\n",
    "all_data = sio.loadmat(cur_file)\n",
    "\n",
    "\n",
    "train_data, test_data, train_label, test_label = generating_data(all_data, label_list, fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import DGCNN\n",
    "from sklearn import preprocessing\n",
    "from utils import generating_data\n",
    "from glob import glob\n",
    "import itertools\n",
    "\n",
    "# lr_dgcnn = 0.001#0.01\n",
    "# weight_decay_dgcnn = 8e-5 # 5e-4tem no codigo do GMSS tmbm\n",
    "#lr_dgcnn = 0.01#0.01\n",
    "#weight_decay_dgcnn = 5e-4 # 5e-4tem no codigo do GMSS tmbm\n",
    "\n",
    "K=2\n",
    "epochs=20 # DEFFERARD TMBM USA 20\n",
    "device = 'cpu'\n",
    "\n",
    "# lr_dgcnn_list = [0.01,0.03, 0.05, 0.07, 0.09]\n",
    "# weight_decay_dgcnn_list = [8e-5,5e-4,5e-4]\n",
    "# out_channel_list = [32,48]\n",
    "\n",
    "# 0.0009,0.001,0.005,0.009,0.01\n",
    "lr_dgcnn_list = [0.009,0.005, 0.003, 0.001] #diminuindo.\n",
    "weight_decay_dgcnn_list = [5e-4,9e-4,0.001,0.005,0.009,0.01] #aumentando\n",
    "out_channel_list = [32,48]\n",
    "\n",
    "all_hiperparams_combination = list(itertools.product(lr_dgcnn_list,weight_decay_dgcnn_list,out_channel_list))\n",
    "# all_hiperparams_combination\n",
    "\n",
    "hiperparam_list = []\n",
    "\n",
    "for lr_dgcnn, weight_decay_dgcnn, out_channel_dgcnn in all_hiperparams_combination:\n",
    "    cur_hiperparam_dict = {\n",
    "        'lr_dgcnn':lr_dgcnn,\n",
    "        'weight_decay_dgcnn':weight_decay_dgcnn,\n",
    "        'K':K,\n",
    "        'epochs':epochs,\n",
    "        'out_channel_dgcnn':out_channel_dgcnn\n",
    "        }\n",
    "    hiperparam_list.append(cur_hiperparam_dict)\n",
    "\n",
    "#hiperparam_list\n",
    "\n",
    "\n",
    "label_list = np.array([1,0,-1,-1,0,1,-1,0,1,1,0,-1,0,1,-1]) + 1 # -1 for \n",
    "#fn = 'de_LDS'\n",
    "feature_list = ['de_LDS']#,'psd_LDS']#, 'rasm_LDS', 'dasm_LDS', 'asm_LDS','dcau_LDS']\n",
    "\n",
    "full_path = \"/home/joaovmrf/Documents/mestrado/SEED/SEED_EEG/SEED_EEG/ExtractedFeatures\"\n",
    "all_files = [each_file for each_file in glob(full_path+'/*.mat') if 'label.mat' not in each_file]\n",
    "all_files.sort()\n",
    "# all_files = glob.glob(full_path+\"*\")\n",
    "\n",
    "all_acc_results = []\n",
    "for hiperparam_run in hiperparam_list:\n",
    "    lr_dgcnn = hiperparam_run['lr_dgcnn']\n",
    "    weight_decay_dgcnn = hiperparam_run['weight_decay_dgcnn']\n",
    "    out_channel_dgcnn = hiperparam_run['out_channel_dgcnn']\n",
    "\n",
    "    for cur_file in all_files:\n",
    "        for fn in feature_list:\n",
    "            #item = \"15_20130709.mat\"\n",
    "            #item = \"13_20140527.mat\"\n",
    "            start_time = datetime.now()\n",
    "\n",
    "            item = os.path.basename(cur_file)\n",
    "            cur_user = item.split(\"_\")[0]\n",
    "            user_session = datetime.strptime(item.split(\"_\")[1][:-4],\"%Y%m%d\")\n",
    "            print(f\"- reading file: {item}\\n--user:{cur_user}, sessions: {user_session}\")\n",
    "\n",
    "            # cur_file = os.path.join(full_path, item)\n",
    "            all_data = sio.loadmat(cur_file)\n",
    "\n",
    "\n",
    "            train_data, test_data, train_label, test_label = generating_data(all_data, label_list, fn)\n",
    "\n",
    "            #train_data = preprocessing.scale(train_data)\n",
    "            #test_data = preprocessing.scale(test_data)\n",
    "\n",
    "            train_data_tensor = torch.from_numpy(train_data).to(torch.float)#.to(device)\n",
    "            test_data_tensor = torch.from_numpy(test_data).to(torch.float)#.to(device)\n",
    "            train_label_tensor = torch.from_numpy(train_label).to(torch.long)#.to(device)\n",
    "            test_label_tensor = torch.from_numpy(test_label).to(torch.long)#.to(device)\n",
    "\n",
    "            # define model\n",
    "            model = DGCNN(\n",
    "                in_channels=5, #5 frequencias\n",
    "                num_electrodes=train_data.shape[1], #depende de cada feature\n",
    "                k_adj=K, #numero de layers - copiando do GMSS - na verdade no GMSS K=ordem de chebyshev\n",
    "                # out_channels=32, #copiando do GMSS, mas ainda nao entendi\n",
    "                out_channels=out_channel_dgcnn,\n",
    "                num_classes=3\n",
    "            )\n",
    "\n",
    "            best_test_res = {\n",
    "                'acc':0,\n",
    "                'predict_label':None,\n",
    "                'trur_label':None\n",
    "            }\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(),\n",
    "                                lr=lr_dgcnn, weight_decay=weight_decay_dgcnn)\n",
    "            myloss = nn.CrossEntropyLoss()\n",
    "            #print(\"matriz de adjacencia inicial:\")\n",
    "            #print(model.A)\n",
    "            for cur_epoch in range(epochs):\n",
    "                # ordem 1\n",
    "                # model.train() # como apontar  referencia aos dados de treino? R: E na definicao do modelo\n",
    "                # optimizer.zero_grad() #etapa comum em todos, zerar o gradiente.\n",
    "\n",
    "                # output = model(train_data_tensor)\n",
    "                # #TODO accuracy.\n",
    "                # #print(output)\n",
    "\n",
    "                # # import torch.nn.functional as F\n",
    "                # # loss_train = F.nll_loss(output, train_label)\n",
    "\n",
    "                # myloss = nn.CrossEntropyLoss()\n",
    "                # loss = myloss(output, train_label_tensor)\n",
    "                # loss.backward()\n",
    "                # optimizer.step()\n",
    "\n",
    "                # ordem 2\n",
    "                model.train() # como apontar  referencia aos dados de treino? R: E na definicao do modelo\n",
    "                output = model(train_data_tensor)\n",
    "                # STEP 10 - Calculate the loss function\n",
    "                loss = myloss(output, train_label_tensor)\n",
    "                \n",
    "                optimizer.zero_grad() #etapa comum em todos, zerar o gradiente.\n",
    "\n",
    "                #TODO accuracy.\n",
    "                #print(output)\n",
    "\n",
    "                # import torch.nn.functional as F\n",
    "                # loss_train = F.nll_loss(output, train_label)\n",
    "                # STEP 11 - Updating the Adj matrix\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Estou sentindo falta do coidgo de atualizacao dessa matriz\n",
    "\n",
    "                train_acc = torch.sum(torch.argmax(output, 1) == train_label_tensor) / train_data_tensor.shape[0]\n",
    "\n",
    "                print('Epoch : {} -- TrainLoss : {} -- TrainAcc : {}\\n'.format(cur_epoch, loss.cpu().data, train_acc.cpu().data))\n",
    "                #print(f\"matriz de adjacencia atualizada {cur_epoch}:\")\n",
    "                #print(model.A)\n",
    "            #TODO print tempo de proc.\n",
    "            # model.eval()\n",
    "            test_output = model(test_data_tensor) # aqui eu preciso puxar a MAtriz de adjancencia do treino, nao comecar uma.\n",
    "            test_loss = myloss(test_output, test_label_tensor)\n",
    "            test_acc = torch.sum(torch.argmax(test_output, 1) == test_label_tensor) / test_data_tensor.shape[0]\n",
    "            print('Epoch : {} -- TestLoss : {} -- TestAcc : {}\\n'.format(cur_epoch, test_loss.cpu().data, test_acc.cpu().data))\n",
    "\n",
    "            # save the best results    \n",
    "            # if best_test_res['acc'] < test_acc.cpu().data:\n",
    "            best_test_res['acc'] = test_acc.cpu().data \n",
    "            best_test_res['loss'] = test_loss.cpu().data \n",
    "            best_test_res['predict_label'] = torch.argmax(test_output, 0).cpu().numpy()\n",
    "            best_test_res['true_label'] = test_label_tensor.cpu().numpy()\n",
    "            print('update res')\n",
    "            end_time = datetime.now()\n",
    "            all_acc_results.append([end_time-start_time,fn,item,best_test_res, cur_file,str(hiperparam_run)])\n",
    "\n",
    "            print(best_test_res)\n",
    "            # TODO logar em um arquivo os resultados totais.\n",
    "            # salvar em TXT:\n",
    "            filename_to_write = 'temp_analise_gcnn.txt'\n",
    "            with open(filename_to_write, \"a+\") as f:\n",
    "                rows_as_string = ';'.join([str(end_time-start_time),item,fn,str(hiperparam_run),str(best_test_res)])\n",
    "                f.write(f\"{rows_as_string}\\n\")\n",
    "\n",
    "            # break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnn_result_df = pd.DataFrame(all_acc_results)\n",
    "gcnn_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(gcnn_result_df[2].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnn_result_df[[\"user\", \"session\"]]  = pd.DataFrame(gcnn_result_df[2].str.split('_').to_list())\n",
    "gcnn_result_df[['acc','predict_label','trur_label','loss','true_label']] = pd.DataFrame(gcnn_result_df[3].to_list())\n",
    "gcnn_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnn_result_df['acc'] = gcnn_result_df['acc'].apply(lambda x: x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcnn_result_df[[0,'user','session','acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teste,pegar as duas maiores acuracias;\n",
    "duas_maiores_acuracias = pd.DataFrame(gcnn_result_df[[0,'user','session','acc']].sort_values([0,'user', 'acc'], ascending=[True,False, False])\\\n",
    "    .groupby([0,'user'])['acc']\\\n",
    "    .nlargest(2)\\\n",
    "        ).reset_index()\n",
    "    #.to_list()\n",
    "    #.agg({'acc':['mean','std']})\n",
    "duas_maiores_acuracias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_row_to_insert = duas_maiores_acuracias.groupby([0]).agg({'acc':['mean','std']}).reset_index()\n",
    "cur_row_to_insert.columns = cur_row_to_insert.columns.droplevel()\n",
    "cur_row_to_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_with_info = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duas_maiores_acuracias.agg({'acc':['mean','std']}).reset_index()\n",
    "# duas_maiores_acuracias.columns = ['_'.join(col) for col in duas_maiores_acuracias.columns]\n",
    "# duas_maiores_acuracias.columns\n",
    "row_with_info = row_with_info.append(cur_row_to_insert)\n",
    "row_with_info.loc[:,'hiperparams'] = str(hiperparam_run)\n",
    "row_with_info.loc[:,'technic'] = '2 maiores'\n",
    "row_with_info.loc[:,'processing_time'] = ''\n",
    "\n",
    "row_with_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acuracia_para_todos.values.flatten()\n",
    "# acuracia_para_todos.loc[:,'hiperparams'] = str(hiperparam_run)\n",
    "# acuracia_para_todos.loc[:,'technic'] = 'todas as sessoes'\n",
    "# acuracia_para_todos.loc[:,'processing_time'] = ''\n",
    "# acuracia_para_todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acuracia_para_todos = gcnn_result_df[[0,'user','session','acc']].sort_values([0,'user', 'acc'], ascending=[True,False, False])\\\n",
    ".groupby([0]).agg({'acc':['mean','std']}).reset_index()\n",
    "acuracia_para_todos.columns = acuracia_para_todos.columns.droplevel(0)\n",
    "acuracia_para_todos.loc[:,'hiperparams'] = str(hiperparam_run)\n",
    "acuracia_para_todos.loc[:,'technic'] = 'todas as sessoes'\n",
    "acuracia_para_todos.loc[:,'processing_time'] = ''\n",
    "\n",
    "cur_row_to_insert = acuracia_para_todos \n",
    "\n",
    "row_with_info = row_with_info.append([cur_row_to_insert])\n",
    "row_with_info['data_da_rodada'] = datetime.now()\n",
    "row_with_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvar em TXT:\n",
    "# filename_to_write = 'analise_gcnn.csv'\n",
    "# with open(filename_to_write, \"a+\") as f:\n",
    "#     rows_as_string = row_with_info.to_string(header=True, index=True)\n",
    "#     f.write(rows_as_string)\n",
    "\n",
    "filename_to_write = './analise_gcnn.csv'\n",
    "row_with_info.to_csv(filename_to_write,mode='a',header=True, index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_gcnn = gcnn_result_df[['user', 'session','acc', 'predict_label','trur_label','loss','true_label']].sort_values(['user', 'acc'], ascending=[False, False])\n",
    "eda_gcnn = gcnn_result_df[['user', 0,'session','acc']].sort_values(['user', 'acc'], ascending=[False, False]).to_csv(f\"GCNN_WITHOUT_EVAL_20230912.csv\")\n",
    "\n",
    "eda_gcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_gcnn.reset_index().to_csv(f\"GCNN_WITHOUT_EVAL_20230912.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_gcnn.groupby(['user', 0,'session']).max()#.acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_gcnn.groupby(['user']).head(2).reset_index(drop=True)#.acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analisar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import DGCNN\n",
    "from sklearn import preprocessing\n",
    "from utils import generating_data\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'de_LDS'\n",
    "\n",
    "label_list = np.array([1,0,-1,-1,0,1,-1,0,1,1,0,-1,0,1,-1]) + 1 # -1 for \n",
    "\n",
    "full_path = \"/home/joaovmrf/Documents/mestrado/SEED/SEED_EEG/SEED_EEG/ExtractedFeatures\"\n",
    "all_files = [each_file for each_file in glob(full_path+'/*.mat') if 'label.mat' not in each_file]\n",
    "all_files.sort()\n",
    "# all_files = glob.glob(full_path+\"*\")\n",
    "\n",
    "\n",
    "# for cur_file in all_files: \n",
    "#item = \"15_20130709.mat\"\n",
    "#item = \"13_20140527.mat\"\n",
    "cur_file = all_files[10]\n",
    "item = os.path.basename(cur_file)\n",
    "cur_user = item.split(\"_\")[0]\n",
    "user_session = datetime.strptime(item.split(\"_\")[1][:-4],\"%Y%m%d\")\n",
    "print(f\"- reading file: {item}\\n--user:{cur_user}, sessions: {user_session}\")\n",
    "\n",
    "# cur_file = os.path.join(full_path, item)\n",
    "all_data = sio.loadmat(cur_file)\n",
    "\n",
    "\n",
    "train_data, test_data, train_label, test_label = generating_data(all_data, label_list, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tensor = torch.from_numpy(test_data).to(torch.float)#.to(device)\n",
    "test_data_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegando apenas o primeiro video (236), todos os canais, a ultima frequencia\n",
    "test_data_tensor[:236,:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegando apenas o primeiro video (236), canal 1, frequencia delta\n",
    "test_data_tensor[:236,0,0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudando eixo para entneder matriz de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_swapped = np.swapaxes(test_data, 0, 1)\n",
    "test_data_swapped = np.swapaxes(test_data_swapped, 1, 2)\n",
    "test_data_swapped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_swapped_tensor = torch.from_numpy(test_data_swapped).to(torch.float)#.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_swapped_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegando todos os 62 canais, apenas frequencia delta \n",
    "test_data_swapped_tensor[:,0,:].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montar dicionario com parametros de treino;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "lr_dgcnn_list = [0.01,0.03, 0.05, 0.07, 0.09]\n",
    "weight_decay_dgcnn_list = [8e-5,5e-4,5e-4]\n",
    "out_channel_list = [32,48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "lr_dgcnn_list = [0.01,0.03, 0.05, 0.07, 0.09]\n",
    "weight_decay_dgcnn_list = [8e-5,5e-4,5e-4]\n",
    "out_channel_list = [32,48]\n",
    "\n",
    "all_hiperparams_combination = list(itertools.product(lr_dgcnn_list,weight_decay_dgcnn_list,out_channel_list))\n",
    "# all_hiperparams_combination\n",
    "\n",
    "hiperparam_list = []\n",
    "\n",
    "for lr_dgcnn, weight_decay_dgcnn, out_channel in all_hiperparams_combination:\n",
    "    cur_hiperparam_dict = {\n",
    "        'lr_dgcnn ':lr_dgcnn,\n",
    "        'weight_decay_dgcnn':weight_decay_dgcnn,\n",
    "        'K':K,\n",
    "        'epochs':epochs,\n",
    "        'out_channel':out_channel\n",
    "        }\n",
    "    hiperparam_list.append(cur_hiperparam_dict)\n",
    "\n",
    "hiperparam_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiperparam_list = []\n",
    "\n",
    "for lr_dgcnn, weight_decay_dgcnn, out_channel in all_hiperparams_combination:\n",
    "    cur_hiperparam_dict = {\n",
    "        'lr_dgcnn ':lr_dgcnn,\n",
    "        'weight_decay_dgcnn':weight_decay_dgcnn,\n",
    "        'K':K,\n",
    "        'epochs':epochs,\n",
    "        'out_channel':out_channel\n",
    "        }\n",
    "    hiperparam_list.append(cur_hiperparam_dict)\n",
    "\n",
    "hiperparam_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste de escrita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_to_write = 'teste.txt'\n",
    "with open(filename_to_write, \"a+\") as f:\n",
    "    rows_as_string =';'.join(['oi', 'teste', '20e22'])\n",
    "    f.write(f\"{rows_as_string}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA DE hiperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_de_hiperparams = pd.read_csv(\"temp_analise_gcnn.txt\", header=None, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_de_hiperparams[[\"user\", \"session\"]]  = pd.DataFrame(eda_de_hiperparams[1].str.split('_').to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_de_hiperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_de_hiperparams[['acc','predict_label','trur_label','loss','true_label']] = \n",
    "# import json\n",
    "# json.loads(eda_de_hiperparams.loc[0,4])\n",
    "\n",
    "import ast\n",
    "# ast.literal_eval(eda_de_hiperparams.loc[0,4])\n",
    "# ast.literal_eval(eda_de_hiperparams.loc[0,4].replace(\"'\",'\"'))\n",
    "\n",
    "eval(eda_de_hiperparams.loc[0,4].replace(\"'\",'\"').replace('tensor', '').replace('array',''))\n",
    "# eda_de_hiperparams.loc[0,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_de_hiperparams[['acc','predict_label','trur_label','loss','true_label']] \n",
    "eda_de_hiperparams['4_as_dict'] = eda_de_hiperparams[4].apply(lambda x: eval(x.replace(\"'\",'\"').replace('tensor', '').replace('array','')))\n",
    "eda_de_hiperparams[['acc','predict_label','trur_label','loss','true_label']] = pd.DataFrame(eda_de_hiperparams['4_as_dict'].to_list())\n",
    "# eda_de_hiperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_de_hiperparams[[2,'user','session', 3, 4,'acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duas_maiores_acc_all = eda_de_hiperparams[[2,'user','session', 3, 4,'acc']]\\\n",
    "    .groupby([3, 'user'])['acc']\\\n",
    "    .nlargest(3)\\\n",
    "    .reset_index()\n",
    "duas_maiores_acc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duas_maiores_acc_all_agg = duas_maiores_acc_all.groupby([3]).agg({'acc':['mean','std']}).reset_index()\\\n",
    "\n",
    "duas_maiores_acc_all_agg.columns = duas_maiores_acc_all_agg.columns.droplevel()\n",
    "duas_maiores_acc_all_agg.sort_values(['mean'], ascending=[False])\\\n",
    "    .to_csv(\"long_result_with_3.csv\",sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dgcnn_list = [0.01,0.03, 0.05, 0.07, 0.09]\n",
    "weight_decay_dgcnn_list = [8e-5,5e-4]\n",
    "out_channel_list = [32,48]\n",
    "\n",
    "all_hiperparams_combination = list(itertools.product(lr_dgcnn_list,weight_decay_dgcnn_list,out_channel_list))\n",
    "all_hiperparams_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppget_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
